{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebd4364",
   "metadata": {},
   "source": [
    "# Exercise - The YouTube Dataset: Classification - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48026b9",
   "metadata": {},
   "source": [
    "### Introducing the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1101f2a",
   "metadata": {},
   "source": [
    "You can complete this exercise only after you have studied the following lessons:\n",
    "* **The YouTube Dataset: Confusion matrix** and\n",
    "* **The YouTube Dataset: Accuracy, Precision, Recall, and the F1 score**\n",
    "\n",
    "In this exercise, you will explore the difference in performance between the Multinomial and Comlpement Naive Bayes classifiers by looking into the confusion matrix and classification report and comparing them with the results from the lessons. \n",
    "\n",
    "The code under the following sections is implemented: \n",
    "* **Importing the necessary libraries** - importi **some** libraries necessary for completing the exercise.\n",
    "* **Reading the database** - concatenatе the datasets from all 5 \".csv\" files into one dataframe and removing the unnecessary columns.\n",
    "* **Defining the inputs and the target. Creating the train-test split.** - definе the inputs as all comments from the database. The targets are their respective classes. Afterwards, split the data into training and testing sets.\n",
    "* **Tokenizing the YouTube comments** - create a vocabulary dictionary containing the words from **training** dataset.\n",
    "\n",
    "Don't forget to put this notebook in the parent folder of the **youtube-dataset** folder. Otherwise, you would need to change the path under **Reading the database**.\n",
    "\n",
    "Please implement the relevant code from these lessons under the following sections:\n",
    "* **Performing the classification** - define a classifier and fit the model to the training data.\n",
    "* **Performing the evaluation on the test dataset** - make predictions on the test data. Using those predictions, construct a confusion matrix and a classification report.\n",
    "\n",
    "Please answer the following questions when you are ready:\n",
    "* What changes do you observe in the confusion matrix when using the Complement Naive Bayes algorithm?\n",
    "* How do these changes affect the classification report?\n",
    "* How do you interpret the results?\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053831be",
   "metadata": {},
   "source": [
    "### Introducing the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b3cb7",
   "metadata": {},
   "source": [
    "The database for this example is taken from https://archive.ics.uci.edu/ml/machine-learning-databases/00380/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85d311",
   "metadata": {},
   "source": [
    "We usually modify the databases slightly such that they fit the purpose of the course. Therefore, we suggest you use the database provided in the resources in order to obtain the same results as the ones in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299f9e4",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59c4c3",
   "metadata": {},
   "source": [
    "### Reading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('youtube-dataset\\\\*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "\n",
    "for i in files:\n",
    "    all_df.append(pd.read_csv(i).drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b015ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(all_df, axis=0, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167043b",
   "metadata": {},
   "source": [
    "### Defining the inputs and the target. Creating the train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['CONTENT']\n",
    "target = data['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs, target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=365, \n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8a85e",
   "metadata": {},
   "source": [
    "### Tokenizing the YouTube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transf = vectorizer.fit_transform(x_train)\n",
    "x_test_transf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cdb00",
   "metadata": {},
   "source": [
    "### Performing the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ComplementNB()\n",
    "\n",
    "clf.fit(x_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d523f",
   "metadata": {},
   "source": [
    "### Performing the evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(x_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344760ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_test_pred,\n",
    "    labels = clf.classes_,\n",
    "    cmap = 'magma'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred, target_names = ['Ham', 'Spam']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
